{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzcIvSLKFvRKMU6IzZfsSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0f0cab140a4433881cd51d51761a526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_40a24d86bf3d43b7848f1b8a56e4d6df",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96d90137b1c546dea79a87f258ec3705",
              "IPY_MODEL_3eb00e11f81442f196f7b3c2eb5658a2"
            ]
          }
        },
        "40a24d86bf3d43b7848f1b8a56e4d6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96d90137b1c546dea79a87f258ec3705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_51265034aa4e4a58a8037626b8491ccc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa4b2b0e943f429d872f35b4070668c4"
          }
        },
        "3eb00e11f81442f196f7b3c2eb5658a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_382ed0dd19d64ecfb62c823070e81584",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 877kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61795fc5c683452a86c9a5930207105a"
          }
        },
        "51265034aa4e4a58a8037626b8491ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa4b2b0e943f429d872f35b4070668c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "382ed0dd19d64ecfb62c823070e81584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61795fc5c683452a86c9a5930207105a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raduga256/SentimentAnalysisUsingBERT/blob/main/bert_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzGkXHQ-IXQ"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpwfb4Di60wi",
        "outputId": "95a1b543-9c6e-4e2a-ed37-28cca8156c30"
      },
      "source": [
        "# checking \n",
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  2 15:27:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDjU-GU-RQ3",
        "outputId": "9359b819-f4c4-41df-d3a6-a5333990e043"
      },
      "source": [
        " # Installing the Tranformers Library v4.0.1\n",
        " !pip -qq install transformers==3.2.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 7.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 26.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 41.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 55.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4n3TmI2bu8H"
      },
      "source": [
        "# Installing a specific version of PyTorch because colab default is 1.4\n",
        "# !pip install --no-cache-dir torch==1.6.0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVj-7uWG_qOS",
        "outputId": "40295d4a-735c-4739-dfc4-e50933da1176"
      },
      "source": [
        "# Installing PyTorch Library v1.2.0\n",
        "!pip install -qq pytorch-lightning   #quite mode"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 839kB 8.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 19.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 48.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 49.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 48.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 35.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 57.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 58.5MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ysrt0EHAF6",
        "outputId": "88f662fa-5261-4bd4-80a3-03331e77485a"
      },
      "source": [
        "# checking for the installed pytorch version\n",
        "print(torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSmxVcLVfRHq"
      },
      "source": [
        "# Intalling torchtext version compatible to installed pytorch v1.6\n",
        "# !pip install -U torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8nf8k79Yz0Y",
        "outputId": "5eea5971-80b8-4d10-f194-3aa4ac675a1a"
      },
      "source": [
        "!pip list | grep torchtext   # What version do you have for torchtext?"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torchtext                     0.9.1         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbGAsxfqARrK"
      },
      "source": [
        "# Import Some Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "# from transformers.optimization import get_linear_scheduler_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional.classification import auroc\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRy_NE9TnsMk"
      },
      "source": [
        "# Seeding the Random Generator and the Torch Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rDpR_4UoOCH",
        "outputId": "2ae52168-3249-4367-ceb8-cfc777e3d396"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = (\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\")\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED) # it is not the best way to do but it will work for this case\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0b4c7fa7d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seKA3tuNbMuD",
        "outputId": "37abdc8c-1a39-43b5-d746-2b1c234c3b4e"
      },
      "source": [
        "# Downoad the data set\n",
        "# dataset_url = \"https://raw.githubusercontent.com/iampukar/toxic-comments-classification/master/sample_submission.csv\"\n",
        "# df1 = pd.read_csv(dataset_url) this dataset doesn't contain original toxic comments so i have ignored it for alternative similar\n",
        "\n",
        "\"\"\"These are google play apps reviews. \"\"\"\n",
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
            "To: /content/apps.csv\n",
            "100% 134k/134k [00:00<00:00, 49.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\n",
            "To: /content/reviews.csv\n",
            "7.17MB [00:00, 63.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "9BHDV4nGnKh-",
        "outputId": "2c15bee9-8497-4b31-a7c5-590f6cd089fc"
      },
      "source": [
        "df = pd.read_csv(\"reviews.csv\")\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars PanzerbjÃ¸rn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ...      appId\n",
              "0     Andrew Thomas  ...  com.anydo\n",
              "1      Craig Haines  ...  com.anydo\n",
              "2     steven adkins  ...  com.anydo\n",
              "3  Lars PanzerbjÃ¸rn  ...  com.anydo\n",
              "4     Scott Prewitt  ...  com.anydo\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKgAjhCvt8OX",
        "outputId": "2f6daa68-2e06-4013-9134-f357e4295d57"
      },
      "source": [
        "# Checking out the dataset shape\n",
        "df.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15746, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdfo4rMR7PQ6",
        "outputId": "3f137d5a-e19a-4db1-a374-a461567e4a9a"
      },
      "source": [
        "# We have about 16k examples. Letâ€™s check for missing values:\n",
        "df.info()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15746 entries, 0 to 15745\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   userName              15746 non-null  object\n",
            " 1   userImage             15746 non-null  object\n",
            " 2   content               15746 non-null  object\n",
            " 3   score                 15746 non-null  int64 \n",
            " 4   thumbsUpCount         15746 non-null  int64 \n",
            " 5   reviewCreatedVersion  13533 non-null  object\n",
            " 6   at                    15746 non-null  object\n",
            " 7   replyContent          7367 non-null   object\n",
            " 8   repliedAt             7367 non-null   object\n",
            " 9   sortOrder             15746 non-null  object\n",
            " 10  appId                 15746 non-null  object\n",
            "dtypes: int64(2), object(9)\n",
            "memory usage: 1.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "C05eWJrC7iyU",
        "outputId": "60892122-5089-46ca-81d7-7ad011706f54"
      },
      "source": [
        "# Great, no missing values in the score and review texts! Do we have class imbalance?\n",
        "sns.countplot(df.score)\n",
        "plt.xlabel('Review Count')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Review Count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 745,
              "height": 489
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-4oV_yo8jZ2"
      },
      "source": [
        "Thatâ€™s hugely imbalanced, but itâ€™s okay. Weâ€™re going to convert the dataset into negative, neutral and positive sentiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ADukLRi8m-5"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "      return 1\n",
        "  else:\n",
        "        return 2\n",
        "\n",
        "df['sentiment'] = df.score.apply(to_sentiment)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7E_mLJF-Cwj"
      },
      "source": [
        "# class labels\n",
        "class_labels = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "bPxaEHxY-agn",
        "outputId": "fc80b1e6-e59d-41b9-fc0f-d87c9000da82"
      },
      "source": [
        "#  plot\n",
        "ax = sns.countplot(df.sentiment)\n",
        "plt.xlabel('review sentiment')\n",
        "ax.set_xticklabels(class_labels);"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 745,
              "height": 489
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1yvluP9_zlY"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVF3Y49AJxk"
      },
      "source": [
        "You might already know that Machine Learning models donâ€™t work with raw text. You need to convert text to numbers (of some sort). BERT requires even more attention (good one, right?). Here are the requirements:\n",
        "\n",
        "Add special tokens to separate sentences and do classification\n",
        "Pass sequences of constant length (introduce padding)\n",
        "Create array of 0s (pad token) and 1s (real token) called attention mask\n",
        "The Transformers library provides (youâ€™ve guessed it) a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers that do the heavy lifting for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-VnmaEzAWiK"
      },
      "source": [
        "# Prebuilt tokenizer\n",
        "#You can use a cased and uncased version of BERT and tokenizer\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e0f0cab140a4433881cd51d51761a526",
            "40a24d86bf3d43b7848f1b8a56e4d6df",
            "96d90137b1c546dea79a87f258ec3705",
            "3eb00e11f81442f196f7b3c2eb5658a2",
            "51265034aa4e4a58a8037626b8491ccc",
            "fa4b2b0e943f429d872f35b4070668c4",
            "382ed0dd19d64ecfb62c823070e81584",
            "61795fc5c683452a86c9a5930207105a"
          ]
        },
        "id": "sqmBQAfo-Eba",
        "outputId": "8927ab6f-f41c-4f6d-b124-642f01492a19"
      },
      "source": [
        "# Let's  load a pre-trained BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0f0cab140a4433881cd51d51761a526",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOEguw4ND7za"
      },
      "source": [
        "# Weâ€™ll use this text to understand the tokenization process:\n",
        "\n",
        "sample_txt = \"When was I last outside? I am stuck at home for two weeks\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2-C0V34Eocf",
        "outputId": "600845ba-7c6e-4cb5-fe06-56dca64c03dd"
      },
      "source": [
        "# Some basic operations can convert the text to tokens and tokens to unique integers (ids)\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Sentence: When was I last outside? I am stuck at home for two weeks\n",
            "   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', 'two', 'weeks']\n",
            "Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 1160, 2277]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGklgs2eGVMj"
      },
      "source": [
        "## Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U5NDtYoGK3b",
        "outputId": "9da2e4fa-1cb5-420e-faac-ad531837b3e0"
      },
      "source": [
        "# -marker for endings of a sentence\n",
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYg4YTYUHAs7",
        "outputId": "0f080e49-947e-4908-a389-420a169cbc89"
      },
      "source": [
        "# we must add this token to the start of each sentence, so BERT knows weâ€™re doing classification\n",
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS]', 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u18dlgH5Hod9"
      },
      "source": [
        "BERT understands tokens that were in the training set. Everything else can be encoded using the [UNK] (unknown) token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6hz4ZD6HfOC",
        "outputId": "7c801596-50fe-4b5c-8df5-0cb557a2dd4f"
      },
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YZH7ZKoIIvM"
      },
      "source": [
        "All of that work above can be done using the encode_plus() method illustrated below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH2aMYr1IRx-",
        "outputId": "424ad093-7ffc-438b-b858-880dfc496d4c"
      },
      "source": [
        "#  encode_plus()\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length=True,\n",
        "    #truncation=True,\n",
        "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "    return_token_type_ids=False,\n",
        "    padding = True, # pad_to_max_length=True, is deprecated\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'  # Return PyTorch tensors   \n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Tg9BHXw3DC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-yIH9qVLLoW",
        "outputId": "8fadaa65-f22a-476a-a021-d7dddbe44940"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
              "        1111, 1160, 2277,  102])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRIy95eMIEg",
        "outputId": "8b3ff111-4543-49bb-b8b2-e180abec6263"
      },
      "source": [
        "# The attention mask has the same length\n",
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask'] "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVKgogzONFar",
        "outputId": "0a5f1c33-da37-4806-f375-44ed28b34761"
      },
      "source": [
        "# We can inverse the tokenization to have a look at the special tokens:\n",
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'When',\n",
              " 'was',\n",
              " 'I',\n",
              " 'last',\n",
              " 'outside',\n",
              " '?',\n",
              " 'I',\n",
              " 'am',\n",
              " 'stuck',\n",
              " 'at',\n",
              " 'home',\n",
              " 'for',\n",
              " 'two',\n",
              " 'weeks',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQf6SphPOfz0"
      },
      "source": [
        "## Choosing Sequence Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVPYcssFOpCk"
      },
      "source": [
        "BERT works with fixed-length sequences. Weâ€™ll use a simple strategy to choose the max length. Letâ€™s store the token length of each review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ECstDpOzS3",
        "outputId": "83c02576-0350-4224-f83a-15c697347c30"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df['content']:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "FgZlCsPDuJxp",
        "outputId": "9496648c-605c-4751-8b6e-8b984f0da37c"
      },
      "source": [
        "# plot the distributions\n",
        "sns.histplot(token_lens, kde=True)\n",
        "plt.xlim([0, 256])\n",
        "plt.xlabel('Token Count')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Token Count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 736,
              "height": 489
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb4kOe9kZrXT"
      },
      "source": [
        "Most of the reviews seem to contain less than 128 tokens, but we'll be on the safe side and choose a maximum length of 160"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXNjjEsZqMh"
      },
      "source": [
        "MAX_LEN = 160"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-_1kgTCaSAI"
      },
      "source": [
        "We have all buildings blocks required to create a PyTorch dataset. Let's do it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX5Ia7B-ZpsC"
      },
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "  def __ini__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    #encoding for tokenizer\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=160, #self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'review_text': review,\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0vbrKrOfI8m"
      },
      "source": [
        "The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so itâ€™ll be easier to evaluate the predictions from our model. Letâ€™s split the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZv-GvHafOUq",
        "outputId": "499dd495-2730-4201-e0d1-f914a1e490cf"
      },
      "source": [
        "df_train, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.1,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "df_val, df_test = train_test_split(\n",
        "    df_test,\n",
        "    test_size=0.5,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14171, 12), (787, 12), (788, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvxd7R1fg2iA"
      },
      "source": [
        "We also need to create a couple of data loaders. Here is a helper function to do it:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yZV5MxAhGr5"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  \n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQIFXPZjkAHS"
      },
      "source": [
        ""
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPsb5FqVkFKu"
      },
      "source": [
        "# Letâ€™s have a look at an example batch from our training data loader:\n",
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}